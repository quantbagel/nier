//! gRPC client for sending frames to the inference service.
//!
//! This module handles communication with the inference service,
//! including connection management, batching, and retry logic.

use crate::config::GrpcConfig;
use crate::frame_processor::ProcessedFrame;
use async_trait::async_trait;
use backoff::{backoff::Backoff, ExponentialBackoff};
use bytes::Bytes;
use parking_lot::RwLock;
use std::sync::atomic::{AtomicBool, AtomicU64, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant};
use thiserror::Error;
use tokio::sync::{mpsc, Semaphore};
use tokio::time::timeout;
use tonic::transport::{Channel, Endpoint};
use tonic::{Request, Status};
use tracing::{debug, error, info, warn};

// Include the generated protobuf types
pub mod proto {
    // In production, this would be generated by tonic-build
    // For now, we define the types manually

    #[derive(Clone, Debug)]
    pub struct Frame {
        pub frame_id: String,
        pub device_id: String,
        pub timestamp_ns: i64,
        pub sequence_number: u64,
        pub width: u32,
        pub height: u32,
        pub pixel_format: String,
        pub data: Vec<u8>,
        pub metadata: Option<FrameMetadata>,
    }

    #[derive(Clone, Debug, Default)]
    pub struct FrameMetadata {
        pub worker_id: String,
        pub zone_id: String,
        pub original_width: u32,
        pub original_height: u32,
        pub source_fps: f32,
        pub source_bitrate_kbps: u32,
    }

    #[derive(Clone, Debug)]
    pub struct SubmitFrameRequest {
        pub frame: Option<Frame>,
        pub priority: u32,
        pub sync: bool,
    }

    #[derive(Clone, Debug)]
    pub struct SubmitFrameResponse {
        pub accepted: bool,
        pub processing_id: String,
        pub estimated_processing_ms: u32,
        pub error_message: String,
    }

    #[derive(Clone, Debug)]
    pub struct SubmitFrameBatchRequest {
        pub frames: Vec<Frame>,
        pub priority: u32,
    }

    #[derive(Clone, Debug)]
    pub struct SubmitFrameBatchResponse {
        pub accepted_count: u32,
        pub rejected_count: u32,
        pub processing_ids: Vec<String>,
    }

    #[derive(Clone, Debug)]
    pub struct HealthCheckRequest {
        pub device_id: String,
    }

    #[derive(Clone, Debug)]
    pub struct HealthCheckResponse {
        pub healthy: bool,
        pub status: String,
    }
}

/// Errors that can occur during gRPC operations.
#[derive(Debug, Error)]
pub enum GrpcError {
    #[error("Connection failed: {0}")]
    ConnectionFailed(String),

    #[error("Request failed: {0}")]
    RequestFailed(String),

    #[error("Request timeout")]
    Timeout,

    #[error("Service unavailable")]
    ServiceUnavailable,

    #[error("Rate limited")]
    RateLimited,

    #[error("Frame rejected: {0}")]
    FrameRejected(String),

    #[error("Channel closed")]
    ChannelClosed,

    #[error("Max retries exceeded")]
    MaxRetriesExceeded,
}

impl From<Status> for GrpcError {
    fn from(status: Status) -> Self {
        match status.code() {
            tonic::Code::Unavailable => GrpcError::ServiceUnavailable,
            tonic::Code::DeadlineExceeded => GrpcError::Timeout,
            tonic::Code::ResourceExhausted => GrpcError::RateLimited,
            _ => GrpcError::RequestFailed(status.message().to_string()),
        }
    }
}

/// Statistics for the gRPC client.
#[derive(Debug, Default, Clone)]
pub struct ClientStats {
    pub frames_sent: u64,
    pub frames_accepted: u64,
    pub frames_rejected: u64,
    pub batches_sent: u64,
    pub total_latency_ms: u64,
    pub avg_latency_ms: f64,
    pub reconnect_count: u32,
    pub last_success_at: Option<Instant>,
    pub last_error_at: Option<Instant>,
}

/// Connection state for the gRPC client.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ClientState {
    Disconnected,
    Connecting,
    Connected,
    Reconnecting,
}

/// Trait for inference service client operations.
#[async_trait]
pub trait InferenceClient: Send + Sync {
    /// Submit a single frame for inference.
    async fn submit_frame(
        &self,
        frame: ProcessedFrame,
        priority: u32,
        sync: bool,
    ) -> Result<SubmitResult, GrpcError>;

    /// Submit a batch of frames for inference.
    async fn submit_batch(
        &self,
        frames: Vec<ProcessedFrame>,
        priority: u32,
    ) -> Result<BatchResult, GrpcError>;

    /// Check health of the inference service.
    async fn health_check(&self, device_id: &str) -> Result<bool, GrpcError>;

    /// Get client statistics.
    fn stats(&self) -> ClientStats;

    /// Get current connection state.
    fn state(&self) -> ClientState;
}

/// Result of a single frame submission.
#[derive(Debug, Clone)]
pub struct SubmitResult {
    pub accepted: bool,
    pub processing_id: String,
    pub estimated_processing_ms: u32,
}

/// Result of a batch frame submission.
#[derive(Debug, Clone)]
pub struct BatchResult {
    pub accepted_count: u32,
    pub rejected_count: u32,
    pub processing_ids: Vec<String>,
}

/// gRPC client for the inference service.
pub struct InferenceGrpcClient {
    config: GrpcConfig,
    channel: Arc<RwLock<Option<Channel>>>,
    state: Arc<RwLock<ClientState>>,
    stats: Arc<RwLock<ClientStats>>,
    running: Arc<AtomicBool>,
    request_semaphore: Arc<Semaphore>,
}

impl InferenceGrpcClient {
    /// Create a new inference gRPC client.
    pub fn new(config: GrpcConfig) -> Self {
        let max_concurrent = config.max_concurrent_requests;

        Self {
            config,
            channel: Arc::new(RwLock::new(None)),
            state: Arc::new(RwLock::new(ClientState::Disconnected)),
            stats: Arc::new(RwLock::new(ClientStats::default())),
            running: Arc::new(AtomicBool::new(false)),
            request_semaphore: Arc::new(Semaphore::new(max_concurrent)),
        }
    }

    /// Connect to the inference service.
    pub async fn connect(&self) -> Result<(), GrpcError> {
        *self.state.write() = ClientState::Connecting;
        self.running.store(true, Ordering::SeqCst);

        let endpoint = Endpoint::from_shared(self.config.inference_endpoint.clone())
            .map_err(|e| GrpcError::ConnectionFailed(e.to_string()))?
            .connect_timeout(self.config.connection_timeout())
            .timeout(self.config.request_timeout());

        // Add TLS configuration if enabled
        // In production, you'd configure TLS here

        let channel = endpoint
            .connect()
            .await
            .map_err(|e| GrpcError::ConnectionFailed(e.to_string()))?;

        *self.channel.write() = Some(channel);
        *self.state.write() = ClientState::Connected;

        info!(
            endpoint = %self.config.inference_endpoint,
            "Connected to inference service"
        );

        Ok(())
    }

    /// Connect with retry logic.
    pub async fn connect_with_retry(&self) -> Result<(), GrpcError> {
        let mut backoff = ExponentialBackoff {
            initial_interval: Duration::from_millis(500),
            max_interval: Duration::from_secs(30),
            max_elapsed_time: None,
            ..Default::default()
        };

        let mut attempts = 0u32;

        loop {
            match self.connect().await {
                Ok(()) => return Ok(()),
                Err(e) => {
                    attempts += 1;
                    self.stats.write().reconnect_count = attempts;

                    if let Some(delay) = backoff.next_backoff() {
                        warn!(
                            endpoint = %self.config.inference_endpoint,
                            attempt = attempts,
                            delay_ms = delay.as_millis(),
                            error = %e,
                            "Connection failed, retrying"
                        );
                        *self.state.write() = ClientState::Reconnecting;
                        tokio::time::sleep(delay).await;
                    } else {
                        error!(
                            endpoint = %self.config.inference_endpoint,
                            attempts = attempts,
                            "Max connection retries exceeded"
                        );
                        return Err(GrpcError::MaxRetriesExceeded);
                    }
                }
            }
        }
    }

    /// Disconnect from the inference service.
    pub async fn disconnect(&self) {
        self.running.store(false, Ordering::SeqCst);
        *self.channel.write() = None;
        *self.state.write() = ClientState::Disconnected;
        info!("Disconnected from inference service");
    }

    /// Get the channel, reconnecting if necessary.
    async fn get_channel(&self) -> Result<Channel, GrpcError> {
        let channel = self.channel.read().clone();

        match channel {
            Some(ch) => Ok(ch),
            None => {
                self.connect_with_retry().await?;
                self.channel
                    .read()
                    .clone()
                    .ok_or(GrpcError::ConnectionFailed("No channel".to_string()))
            }
        }
    }

    /// Convert a ProcessedFrame to the proto Frame type.
    fn frame_to_proto(frame: &ProcessedFrame) -> proto::Frame {
        proto::Frame {
            frame_id: frame.frame_id.clone(),
            device_id: frame.device_id.clone(),
            timestamp_ns: frame.captured_at.elapsed().as_nanos() as i64,
            sequence_number: frame.sequence,
            width: frame.width,
            height: frame.height,
            pixel_format: frame.pixel_format.clone(),
            data: frame.data.to_vec(),
            metadata: Some(proto::FrameMetadata {
                original_width: frame.original_width,
                original_height: frame.original_height,
                ..Default::default()
            }),
        }
    }
}

#[async_trait]
impl InferenceClient for InferenceGrpcClient {
    async fn submit_frame(
        &self,
        frame: ProcessedFrame,
        priority: u32,
        sync: bool,
    ) -> Result<SubmitResult, GrpcError> {
        // Acquire semaphore permit for rate limiting
        let _permit = self
            .request_semaphore
            .acquire()
            .await
            .map_err(|_| GrpcError::ChannelClosed)?;

        let start = Instant::now();
        let frame_id = frame.frame_id.clone();

        let _channel = self.get_channel().await?;

        // In production, this would call the actual gRPC method
        // For now, we simulate the call
        let request = proto::SubmitFrameRequest {
            frame: Some(Self::frame_to_proto(&frame)),
            priority,
            sync,
        };

        debug!(
            frame_id = %frame_id,
            priority = priority,
            sync = sync,
            "Submitting frame to inference service"
        );

        // Simulate gRPC call
        // In production: let response = client.submit_frame(Request::new(request)).await?;
        let response = proto::SubmitFrameResponse {
            accepted: true,
            processing_id: format!("proc-{}", frame_id),
            estimated_processing_ms: 50,
            error_message: String::new(),
        };

        let latency = start.elapsed().as_millis() as u64;

        // Update stats
        {
            let mut stats = self.stats.write();
            stats.frames_sent += 1;
            if response.accepted {
                stats.frames_accepted += 1;
                stats.last_success_at = Some(Instant::now());
            } else {
                stats.frames_rejected += 1;
                stats.last_error_at = Some(Instant::now());
            }
            stats.total_latency_ms += latency;
            stats.avg_latency_ms = stats.total_latency_ms as f64 / stats.frames_sent as f64;
        }

        if !response.accepted {
            return Err(GrpcError::FrameRejected(response.error_message));
        }

        Ok(SubmitResult {
            accepted: response.accepted,
            processing_id: response.processing_id,
            estimated_processing_ms: response.estimated_processing_ms,
        })
    }

    async fn submit_batch(
        &self,
        frames: Vec<ProcessedFrame>,
        priority: u32,
    ) -> Result<BatchResult, GrpcError> {
        if frames.is_empty() {
            return Ok(BatchResult {
                accepted_count: 0,
                rejected_count: 0,
                processing_ids: vec![],
            });
        }

        let _permit = self
            .request_semaphore
            .acquire()
            .await
            .map_err(|_| GrpcError::ChannelClosed)?;

        let start = Instant::now();
        let batch_size = frames.len();

        let _channel = self.get_channel().await?;

        let request = proto::SubmitFrameBatchRequest {
            frames: frames.iter().map(Self::frame_to_proto).collect(),
            priority,
        };

        debug!(
            batch_size = batch_size,
            priority = priority,
            "Submitting frame batch to inference service"
        );

        // Simulate gRPC call
        let response = proto::SubmitFrameBatchResponse {
            accepted_count: batch_size as u32,
            rejected_count: 0,
            processing_ids: frames
                .iter()
                .map(|f| format!("proc-{}", f.frame_id))
                .collect(),
        };

        let latency = start.elapsed().as_millis() as u64;

        // Update stats
        {
            let mut stats = self.stats.write();
            stats.frames_sent += batch_size as u64;
            stats.frames_accepted += response.accepted_count as u64;
            stats.frames_rejected += response.rejected_count as u64;
            stats.batches_sent += 1;
            stats.total_latency_ms += latency;
            stats.avg_latency_ms = stats.total_latency_ms as f64 / stats.batches_sent as f64;
            stats.last_success_at = Some(Instant::now());
        }

        Ok(BatchResult {
            accepted_count: response.accepted_count,
            rejected_count: response.rejected_count,
            processing_ids: response.processing_ids,
        })
    }

    async fn health_check(&self, device_id: &str) -> Result<bool, GrpcError> {
        let _channel = self.get_channel().await?;

        let request = proto::HealthCheckRequest {
            device_id: device_id.to_string(),
        };

        // Simulate health check
        let response = proto::HealthCheckResponse {
            healthy: true,
            status: "OK".to_string(),
        };

        Ok(response.healthy)
    }

    fn stats(&self) -> ClientStats {
        self.stats.read().clone()
    }

    fn state(&self) -> ClientState {
        *self.state.read()
    }
}

/// Batching layer for efficient frame submission.
pub struct BatchingClient {
    inner: Arc<dyn InferenceClient>,
    config: GrpcConfig,
    batch_buffer: Arc<RwLock<Vec<ProcessedFrame>>>,
    running: Arc<AtomicBool>,
}

impl BatchingClient {
    /// Create a new batching client wrapper.
    pub fn new(inner: Arc<dyn InferenceClient>, config: GrpcConfig) -> Self {
        Self {
            inner,
            config,
            batch_buffer: Arc::new(RwLock::new(Vec::new())),
            running: Arc::new(AtomicBool::new(false)),
        }
    }

    /// Start the batching client with a receiver for frames.
    pub async fn run(&self, mut input: mpsc::Receiver<ProcessedFrame>) {
        self.running.store(true, Ordering::SeqCst);

        let batch_timeout = self.config.batch_timeout();
        let batch_size = self.config.batch_size;

        info!(
            batch_size = batch_size,
            timeout_ms = batch_timeout.as_millis(),
            "Batching client started"
        );

        let mut batch: Vec<ProcessedFrame> = Vec::with_capacity(batch_size);
        let mut last_flush = Instant::now();

        loop {
            let should_flush = batch.len() >= batch_size
                || (!batch.is_empty() && last_flush.elapsed() >= batch_timeout);

            if should_flush {
                self.flush_batch(&mut batch).await;
                last_flush = Instant::now();
            }

            // Try to receive with timeout
            match timeout(batch_timeout, input.recv()).await {
                Ok(Some(frame)) => {
                    batch.push(frame);
                }
                Ok(None) => {
                    // Channel closed
                    if !batch.is_empty() {
                        self.flush_batch(&mut batch).await;
                    }
                    break;
                }
                Err(_) => {
                    // Timeout - flush if we have frames
                    if !batch.is_empty() {
                        self.flush_batch(&mut batch).await;
                        last_flush = Instant::now();
                    }
                }
            }

            if !self.running.load(Ordering::SeqCst) {
                break;
            }
        }

        // Flush remaining frames
        if !batch.is_empty() {
            self.flush_batch(&mut batch).await;
        }

        info!("Batching client stopped");
    }

    /// Flush the current batch.
    async fn flush_batch(&self, batch: &mut Vec<ProcessedFrame>) {
        if batch.is_empty() {
            return;
        }

        let frames: Vec<ProcessedFrame> = batch.drain(..).collect();
        let count = frames.len();

        match self.inner.submit_batch(frames, 0).await {
            Ok(result) => {
                debug!(
                    accepted = result.accepted_count,
                    rejected = result.rejected_count,
                    "Batch submitted successfully"
                );
            }
            Err(e) => {
                error!(
                    batch_size = count,
                    error = %e,
                    "Failed to submit batch"
                );
            }
        }
    }

    /// Stop the batching client.
    pub fn stop(&self) {
        self.running.store(false, Ordering::SeqCst);
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Instant;

    fn create_test_config() -> GrpcConfig {
        GrpcConfig {
            inference_endpoint: "http://localhost:50051".to_string(),
            request_timeout_secs: 30,
            connection_timeout_secs: 5,
            max_concurrent_requests: 10,
            use_tls: false,
            ca_cert_path: None,
            enable_compression: false,
            batch_size: 4,
            batch_timeout_ms: 100,
        }
    }

    fn create_test_frame() -> ProcessedFrame {
        ProcessedFrame {
            frame_id: "test-frame-1".to_string(),
            device_id: "test-device".to_string(),
            data: Bytes::from(vec![0u8; 640 * 480 * 3]),
            width: 640,
            height: 480,
            pixel_format: "RGB24".to_string(),
            original_width: 1280,
            original_height: 720,
            sequence: 1,
            captured_at: Instant::now(),
            processed_at: Instant::now(),
            processing_latency_us: 1000,
        }
    }

    #[test]
    fn test_client_creation() {
        let config = create_test_config();
        let client = InferenceGrpcClient::new(config);
        assert_eq!(client.state(), ClientState::Disconnected);
    }

    #[test]
    fn test_frame_to_proto_conversion() {
        let frame = create_test_frame();
        let proto = InferenceGrpcClient::frame_to_proto(&frame);

        assert_eq!(proto.frame_id, "test-frame-1");
        assert_eq!(proto.device_id, "test-device");
        assert_eq!(proto.width, 640);
        assert_eq!(proto.height, 480);
    }

    #[test]
    fn test_stats_default() {
        let config = create_test_config();
        let client = InferenceGrpcClient::new(config);
        let stats = client.stats();

        assert_eq!(stats.frames_sent, 0);
        assert_eq!(stats.frames_accepted, 0);
        assert_eq!(stats.batches_sent, 0);
    }
}
